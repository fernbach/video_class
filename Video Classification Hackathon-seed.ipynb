{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Classification with Apache MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we research and develop architectures for video classification.\n",
    "\n",
    "We use the UTKinect-Action3D Dataset: http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html, that features 10 classes and 199 records.\n",
    "\n",
    "The dataset is provided as (1) zipped file of folders of pictures, which are frames of videos containing multiple actions and (2) a text label file providing mapping between picture names and action. So a prominent first step is to build a pipeline that can associate an action - as a sequence of contiguous frames - and a label.\n",
    "\n",
    "Citation:\n",
    "* title: View invariant human action recognition using histograms of 3D joints,\n",
    "* author: Xia, L. and Chen, C.C. and Aggarwal, JK,\n",
    "* booktitle: Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on,\n",
    " * pages={20--27},\n",
    " * year={2012},\n",
    " * organization={IEEE}\n",
    " \n",
    " \n",
    "Ideas for improvements:\n",
    "* data augmentation\n",
    "* different features:\n",
    "  * add/replace raw data with motion features (eg optical flow)\n",
    "  * use a pre-trained pose estimation model for feature generation (https://gluon-cv.mxnet.io/build/examples_pose/demo_simple_pose.html)\n",
    "  * other classification backbone?\n",
    "* better temporal model (LSTM, conv, ...)\n",
    "* pre-crop around the person with a detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download videoS\n",
    "! wget http://cvrc.ece.utexas.edu/KinectDatasets/RGB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowload labels\n",
    "! wget http://cvrc.ece.utexas.edu/KinectDatasets/actionLabel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip RGB.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couple examples of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import image, nd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "f, axarr = plt.subplots(1, 4)\n",
    "\n",
    "for i, item in enumerate(['colorImg572.jpg', 'colorImg584.jpg', 'colorImg590.jpg', 'colorImg600.jpg']):\n",
    "    \n",
    "    axarr[i].imshow(image.imread('RGB/s01_e01/' + item).asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 4)\n",
    "\n",
    "for i, item in enumerate(['colorImg2110.jpg','colorImg2118.jpg','colorImg2122.jpg','colorImg2128.jpg']):\n",
    "    \n",
    "    axarr[i].imshow(image.imread('RGB/s01_e01/' + item).asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean metadata and prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the gluon approach, that creates a data pipeline with a `DataLoader` on top of a `Dataset`. The Dataset is a simple abstraction to carry records of arbitrary composition (could be some pandas, some numpy, some complex mix of text and images etc) and the `DataLoader` takes those `Dataset` records and moves them to the appropriate context, while doing transformations and batchification in the process.\n",
    "https://mxnet.incubator.apache.org/versions/master/tutorials/gluon/datasets.html\n",
    "\n",
    "Here, data is separated from its labels and the data grain in raw storage is not the same as the data grain we'd like to model so we need to do some pre-processing facilitate the creation of a `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import readline\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, image, nd, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a rich metadata `DataFrame` with labels and data details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('actionLabel.txt')\n",
    "lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('actionLabel.txt', 'r') as f:\n",
    "\n",
    "    labels = []\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.rstrip()\n",
    "        if line.startswith('s') and line.find('_') > -1:\n",
    "            path = line\n",
    "        else:\n",
    "            if \"NaN\" in line or line == \"\":\n",
    "                continue\n",
    "            framestr = line[line.find(':')+1:].strip()\n",
    "            start = int(framestr[:framestr.find(' ')])\n",
    "            stop = int(framestr[framestr.find(' '):])\n",
    "\n",
    "            # count frames\n",
    "            allpics = [re.sub(\"[^0-9]\", \"\", pic) for pic in os.listdir('RGB/' + str(path))]\n",
    "            allpics = [int(pic) for pic in allpics if pic != '']\n",
    "\n",
    "            fct = len([f for f in allpics if start <= f <= stop])\n",
    "\n",
    "            labels.append([line[:line.find(':')], path, start, stop, fct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotdf = pd.DataFrame(labels, columns=['label', 'path', 'fstart', 'fstop', 'frames'])\n",
    "\n",
    "labelmap = {i: L for i, L in enumerate(annotdf['label'].unique())}\n",
    "\n",
    "# add label ID to the annotations dataframe\n",
    "annotdf['labelid'] = annotdf['label'].map(lambda x: [k for k in labelmap if labelmap[k] == x][0])\n",
    "\n",
    "with open('labelmap.json', 'w') as LM:\n",
    "    json.dump(labelmap, LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle, and split train-test the label file. We'll use that split to create associated gluon Datasets and loaders\n",
    "\n",
    "sampledf = annotdf.sample(frac=1, random_state=1)\n",
    "\n",
    "ttsplit = [0.60, 0.30]  # %train, %val, remaining is for test\n",
    "\n",
    "traindf = annotdf.iloc[:int(len(sampledf)*ttsplit[0])]\n",
    "valdf = annotdf.iloc[int(len(sampledf)*ttsplit[0]):int(len(sampledf)*(ttsplit[0] + ttsplit[1]))]\n",
    "testdf = annotdf.iloc[int(len(sampledf)*(ttsplit[0] + ttsplit[1])):]\n",
    "\n",
    "traindf.reset_index(drop=True).to_csv('train_labels.csv', index=False)\n",
    "valdf.reset_index(drop=True).to_csv('val_labels.csv', index=False)\n",
    "testdf.reset_index(drop=True).to_csv('test_labels.csv', index=False)\n",
    "\n",
    "print(str(len(traindf)) + ' training samples')\n",
    "print(str(len(valdf)) + ' validation samples')\n",
    "print(str(len(testdf)) + ' test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a gluon `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prep(img):\n",
    "    \"\"\"prepare an RGB image NDArray for ImageNet pre-trained inference\"\"\"\n",
    "    data = mx.image.resize_short(img, 256).astype('float32')\n",
    "    data, _ = mx.image.center_crop(data, (224,224))\n",
    "    data = data.transpose((2,0,1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSeqDataset(gluon.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset to handle the UTK image sequence dataset json file\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metadata, folder='RGB', downsample=1, framecount=6):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        folder: folder storing images. This is the folder containing the path mentioned in the metadata file\n",
    "        metadata: action metadata file as structured above\n",
    "        downsample: downsample factor\n",
    "        framecount: how many frames to keep. Crop after that limit\n",
    "        records: index of records to select. Use this for train-test split\n",
    "        \"\"\"\n",
    "        self.folder = folder\n",
    "        self.ds = downsample\n",
    "        self.fct = framecount\n",
    "        \n",
    "        self.annotdf = pd.read_csv(metadata)\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        idx: int, index requested\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor: nd.NDArray of seq of images\n",
    "        label: np.NDArray bounding box labels of the form [[x1,y1, x2, y2, class], ...]\n",
    "        \"\"\"\n",
    "        \n",
    "        picdir = self.folder + '/' + self.annotdf.loc[idx]['path']\n",
    "        \n",
    "        # list available frames\n",
    "        pathframes = [int(re.sub(\"[^0-9]\", \"\", pic)) \n",
    "            for pic in os.listdir(picdir)]\n",
    "        \n",
    "        allframes = [f for f in pathframes if (self.annotdf.iloc[idx]['fstart'] <= f \n",
    "                                               and f <= self.annotdf.iloc[idx]['fstop'])]\n",
    "        allframes.sort()\n",
    "        \n",
    "        frames = allframes[:: self.ds][:self.fct]\n",
    "        pics = ['colorImg' + str(f) + '.jpg' for f in frames]\n",
    "        \n",
    "        # if not enough frames, repeating the last one\n",
    "        while len(pics) < self.fct:\n",
    "            pics = pics + [pics[-1]]\n",
    "        \n",
    "        # return a tensor with all prepared images concatenated\n",
    "        tensor = nd.concat(*[nd.expand_dims(img_prep(image.imread(picdir + '/' + pic)), axis=0)\n",
    "                   for pic in pics], dim=0)\n",
    "        \n",
    "        labelid = self.annotdf.iloc[idx]['labelid']\n",
    "\n",
    "        return tensor, labelid\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.annotdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingClassifier(gluon.HybridBlock):\n",
    "    \"\"\"this network runs a softmax on top of the average-pooled frame imagenet embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, backbone, fc_width, ctx, dropout_p=0.3):\n",
    "        \n",
    "        super(PoolingClassifier, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = backbone\n",
    "        self.fc_width = fc_width\n",
    "        self.dropout_p = dropout_p\n",
    "               \n",
    "        with self.name_scope():\n",
    "            \n",
    "            self.emb = models.get_model(name=self.backbone, ctx=ctx, pretrained=True).features\n",
    "            self.dropout_1 = gluon.nn.Dropout(self.dropout_p)\n",
    "            self.fc1 = gluon.nn.Dense(self.fc_width, activation='relu')\n",
    "            self.fc2 = gluon.nn.Dense(self.num_classes)\n",
    "\n",
    "                \n",
    "    def hybrid_forward(self, F, x):\n",
    "        \n",
    "        emb = F.concat(*[F.max(self.emb(ts), axis=0).expand_dims(axis=0) for ts in x], dim=0)\n",
    "        \n",
    "        e1 = self.fc1(emb)\n",
    "        e1 = self.dropout_1(e1)\n",
    "        Y = self.fc2(e1)\n",
    "        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciate data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framecount = 10\n",
    "dsp = 3\n",
    "workers = 6\n",
    "\n",
    "# datasets\n",
    "trainset = ImageSeqDataset(downsample=dsp, framecount=framecount, metadata='train_labels.csv')\n",
    "valset = ImageSeqDataset(downsample=dsp, framecount=framecount, metadata='val_labels.csv')\n",
    "testset = ImageSeqDataset(downsample=dsp, framecount=framecount, metadata='test_labels.csv')\n",
    "\n",
    "# dataloaders\n",
    "train_loader = gluon.data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=workers)\n",
    "\n",
    "val_loader = gluon.data.DataLoader(\n",
    "    dataset=valset,\n",
    "    batch_size=len(valset),\n",
    "    last_batch='rollover',\n",
    "    shuffle=False,\n",
    "    num_workers=workers)\n",
    "\n",
    "test_loader = gluon.data.DataLoader(\n",
    "    dataset=testset,\n",
    "    batch_size=len(testset),\n",
    "    last_batch='rollover',\n",
    "    shuffle=False,\n",
    "    num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PoolingClassifier(num_classes=10, backbone='resnet18_v2', ctx=mx.gpu(), fc_width=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx=mx.gpu()\n",
    "\n",
    "net.fc1.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "net.fc2.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "net.collect_params().reset_ctx(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the resnet part\n",
    "#for p in net.collect_params().items():\n",
    "#    if p[0].find('resnet') > -1:\n",
    "#        p[1].grad_req = 'null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check parameter count\n",
    "net.summary(mx.nd.random.uniform(shape=(1, framecount, 3, 224, 224)).as_in_context(ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(\n",
    "    params=net.collect_params(),\n",
    "    optimizer=mx.optimizer.create('adam', multi_precision=True, learning_rate=0.001))\n",
    "\n",
    "metric = mx.metric.Accuracy()\n",
    "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf(loader, net):\n",
    "    \n",
    "    testmetric = mx.metric.Accuracy()\n",
    "    for inputs, labels in loader:\n",
    "        # Possibly copy inputs and labels to the GPU\n",
    "        inputs = inputs.astype('float16').as_in_context(ctx)\n",
    "        labels = labels.astype('float16').as_in_context(ctx)\n",
    "        testmetric.update(labels, net(inputs))\n",
    "        \n",
    "    _, value = testmetric.get()    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.cast('float16')\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        # Possibly copy inputs and labels to the GPU\n",
    "        inputs = inputs.astype('float16').as_in_context(ctx)\n",
    "        labels = labels.astype('float16').as_in_context(ctx)\n",
    "\n",
    "        with autograd.record():\n",
    "            outputs = net(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Compute gradients by backpropagation and update the evaluation metric\n",
    "        loss.backward()\n",
    "        metric.update(labels, outputs)\n",
    "\n",
    "        # Update the parameters by stepping the trainer; the batch size\n",
    "        # is required to normalize the gradients by `1 / batch_size`.\n",
    "        trainer.step(batch_size=inputs.shape[0])\n",
    "\n",
    "    # Print the evaluation metric and reset it for the next epoch\n",
    "    name, acc = metric.get()\n",
    "    print('After {} epoch : {} = {}'.format(epoch + 1, name, acc))\n",
    "    train_acc.append(acc)\n",
    "    \n",
    "    metric.reset()\n",
    "    \n",
    "    val_perf = perf(val_loader, net)\n",
    "    print(val_perf)\n",
    "    val_acc.append(val_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(train_acc, val_acc)), columns=['train', 'val']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script mode version (optional, wip)\n",
    "Using SageMaker is a good idea to iterate faster. Below is a first draft of a Script Mode compatible script (out of sync with the above code and not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile VideoClassif.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import readline\n",
    "import subprocess as sb\n",
    "import sys\n",
    "\n",
    "sb.call([sys.executable, '-m', 'pip', 'install', '-U', 'pandas'])\n",
    "sb.call([sys.executable, \"-m\", \"pip\", \"install\", \"awscli\"]) \n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, image, nd, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def img_prep(img):\n",
    "    \"\"\"prepare an RGB image NDArray for ImageNet pre-trained inference\"\"\"\n",
    "    data = mx.image.resize_short(img, 256).astype('float32')\n",
    "    data, _ = mx.image.center_crop(data, (224,224))\n",
    "    data = data.transpose((2,0,1))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ImageSeqDataset(gluon.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset to handle the UTK image sequence dataset json file\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metadata, folder='RGB', downsample=1, framecount=6):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        folder: folder storing images. This is the folder containing the path mentioned in the metadata file\n",
    "        metadata: action metadata file as structured above\n",
    "        downsample: downsample factor\n",
    "        framecount: how many frames to keep. Crop after that limit\n",
    "        records: index of records to select. Use this for train-test split\n",
    "        \"\"\"\n",
    "        self.folder = folder\n",
    "        self.ds = downsample\n",
    "        self.fct = framecount\n",
    "        \n",
    "        self.annotdf = pd.read_csv(metadata)\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        idx: int, index requested\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor: nd.NDArray of seq of images\n",
    "        label: np.NDArray bounding box labels of the form [[x1,y1, x2, y2, class], ...]\n",
    "        \"\"\"\n",
    "        \n",
    "        picdir = self.folder + '/' + self.annotdf.loc[idx]['path']\n",
    "        \n",
    "        # list available frames\n",
    "        pathframes = [int(re.sub(\"[^0-9]\", \"\", pic)) \n",
    "            for pic in os.listdir(picdir)]\n",
    "        \n",
    "        allframes = [f for f in pathframes if (self.annotdf.iloc[idx]['fstart'] <= f \n",
    "                                               and f <= self.annotdf.iloc[idx]['fstop'])]\n",
    "        allframes.sort()\n",
    "        \n",
    "        frames = allframes[:: self.ds][:self.fct]\n",
    "        pics = ['colorImg' + str(f) + '.jpg' for f in frames]\n",
    "        \n",
    "        # if not enough frames, repeating the last one\n",
    "        while len(pics) < self.fct:\n",
    "            pics = pics + [pics[-1]]\n",
    "        \n",
    "        \n",
    "        # return a tensor with all prepared images concatenated\n",
    "        tensor = nd.concat(*[nd.expand_dims(img_prep(image.imread(picdir + '/' + pic)), axis=0)\n",
    "                   for pic in pics], dim=0)\n",
    "        \n",
    "        labelid = self.annotdf.iloc[idx]['labelid']\n",
    "\n",
    "        return tensor, labelid\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.annotdf)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class PoolingClassifier(gluon.HybridBlock):\n",
    "    \"\"\"this network runs a softmax on top of the average-pooled frame imagenet embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, backbone, fc_width, ctx, dropout_p=0.3):\n",
    "        \n",
    "        super(PoolingClassifier, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = backbone\n",
    "        self.fc_width = fc_width\n",
    "        self.dropout_p = dropout_p\n",
    "               \n",
    "        with self.name_scope():\n",
    "            \n",
    "            self.emb = models.get_model(name=self.backbone, ctx=ctx, pretrained=True).features\n",
    "            self.dropout_1 = gluon.nn.Dropout(self.dropout_p)\n",
    "            self.dropout_2 = gluon.nn.Dropout(self.dropout_p)\n",
    "            self.fc1 = gluon.nn.Dense(self.fc_width, activation='relu')\n",
    "            self.fc2 = gluon.nn.Dense(self.num_classes)\n",
    "\n",
    "                \n",
    "    def hybrid_forward(self, F, x):\n",
    "        \n",
    "        emb = F.concat(*[F.mean(self.emb(ts), axis=0).expand_dims(axis=0) for ts in x], dim=0)\n",
    "        \n",
    "        e1 = self.fc1(emb)\n",
    "        e1 = self.dropout_1(e1)\n",
    "        Y = self.fc2(e1)\n",
    "        \n",
    "        return Y\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def perf(loader, net):\n",
    "    \n",
    "    testmetric = mx.metric.Accuracy()\n",
    "    for inputs, labels in loader:\n",
    "        # Possibly copy inputs and labels to the GPU\n",
    "        inputs = inputs.astype('float16').as_in_context(ctx)\n",
    "        labels = labels.astype('float16').as_in_context(ctx)\n",
    "        testmetric.update(labels, net(inputs))\n",
    "    \n",
    "    return testmetric.get()\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # extract parameters\n",
    "    # avoid error when running locally with non-declared os.environ['SM_MODEL_DIR']\n",
    "    \n",
    "    parser.add_argument('--backbone', type=str, default='resnet18_v2')\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--ttsplit', type=float, default=0.7)\n",
    "    parser.add_argument('--frames', type=int, default=10)\n",
    "    parser.add_argument('--downsample', type=int, default=2)\n",
    "    parser.add_argument('--batch', type=int, default=16)\n",
    "    parser.add_argument('--loadworkers', type=int, default=6)\n",
    "    parser.add_argument('--fc', type=int, default=32)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--dropout', type=float, default=0.3)\n",
    "\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # DOWNLOAD DATA ----------------------\n",
    "    # images\n",
    "    sb.call(['aws', 's3', 'cp', 's3://cruchant-sm-eu-west1/sagemaker/videoclassif/kinect/RGB.zip', '.'])\n",
    "    # labels\n",
    "    sb.call(['wget', 'http://cvrc.ece.utexas.edu/KinectDatasets/actionLabel.txt'])\n",
    "\n",
    "    sb.call(['unzip', 'RGB.zip'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # PREPARE METADATA -------------------\n",
    "    # read raw annotations and produce a clean annot file\n",
    "    with open('actionLabel.txt', 'r') as f:\n",
    "\n",
    "        labels = []\n",
    "        lines = f.readlines()\n",
    "    \n",
    "        for line in lines:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith('s') and line.find('_') > -1:\n",
    "                path = line\n",
    "            else:\n",
    "                if \"NaN\" in line or line == \"\":\n",
    "                    continue\n",
    "                framestr = line[line.find(':')+1:].strip()\n",
    "                start = int(framestr[:framestr.find(' ')])\n",
    "                stop = int(framestr[framestr.find(' '):])\n",
    "    \n",
    "                # count frames\n",
    "                allpics = [re.sub(\"[^0-9]\", \"\", pic) for pic in os.listdir('RGB/' + str(path))]\n",
    "                allpics = [int(pic) for pic in allpics if pic != '']\n",
    "    \n",
    "                fct = len([f for f in allpics if start <= f <= stop])\n",
    "    \n",
    "                labels.append([line[:line.find(':')], path, start, stop, fct])\n",
    "            \n",
    "    \n",
    "    annotdf = pd.DataFrame(labels, columns=['label', 'path', 'fstart', 'fstop', 'frames'])\n",
    "\n",
    "    labelmap = {i: L for i, L in enumerate(annotdf['label'].unique())}\n",
    "    \n",
    "    # add label ID to the annotations dataframe\n",
    "    annotdf['labelid'] = annotdf['label'].map(lambda x: [k for k in labelmap if labelmap[k] == x][0])\n",
    "    \n",
    "    with open('labelmap.json', 'w') as LM:\n",
    "        json.dump(labelmap, LM)\n",
    "\n",
    "        \n",
    "    # shuffle, and split train-test\n",
    "    \n",
    "    sampledf = annotdf.sample(frac=1, random_state=1)\n",
    "    \n",
    "    ttsplit = [0.75, 0.20]  # %train, %val, remaining is for test\n",
    "    \n",
    "    traindf = annotdf.iloc[:int(len(sampledf)*ttsplit[0])]\n",
    "    valdf = annotdf.iloc[int(len(sampledf)*ttsplit[0]):int(len(sampledf)*(ttsplit[0] + ttsplit[1]))]\n",
    "    testdf = annotdf.iloc[int(len(sampledf)*(ttsplit[0] + ttsplit[1])):]\n",
    "    \n",
    "    traindf.reset_index(drop=True).to_csv('train_labels.csv', index=False)\n",
    "    valdf.reset_index(drop=True).to_csv('val_labels.csv', index=False)\n",
    "    testdf.reset_index(drop=True).to_csv('test_labels.csv', index=False)\n",
    "    \n",
    "    print('Annotations shape: ' + str(annotdf.shape))\n",
    "    \n",
    "    print(str(len(traindf)) + ' training samples')\n",
    "    print(str(len(valdf)) + ' validation samples')\n",
    "    print(str(len(testdf)) + ' test samples')\n",
    "    \n",
    "    \n",
    "    # INSTANCIATE DATA PIPELINE ----------------\n",
    "    framecount = args.frames\n",
    "\n",
    "    # datasets\n",
    "    trainset = ImageSeqDataset(downsample=args.downsample, framecount=framecount, metadata='train_labels.csv')\n",
    "    testset = ImageSeqDataset(downsample=args.downsample, framecount=framecount, metadata='test_labels.csv')\n",
    "    \n",
    "    # dataloaders\n",
    "    train_loader = gluon.data.DataLoader(\n",
    "        dataset=trainset,\n",
    "        batch_size=args.batch,\n",
    "        shuffle=True,\n",
    "        num_workers=args.loadworkers)\n",
    "    \n",
    "    test_loader = gluon.data.DataLoader(\n",
    "        dataset=testset,\n",
    "        batch_size=len(testset),\n",
    "        last_batch='rollover',\n",
    "        shuffle=False,\n",
    "        num_workers=args.loadworkers)\n",
    "    \n",
    "    net = PoolingClassifier(num_classes=10, backbone=args.backbone, ctx=mx.gpu(), fc_width=args.fc, dropout_p=args.dropout)\n",
    "    \n",
    "    ctx=mx.gpu()\n",
    "\n",
    "    net.fc1.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    net.fc2.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    net.collect_params().reset_ctx(ctx)\n",
    "    \n",
    "    trainer = gluon.Trainer(\n",
    "    params=net.collect_params(),\n",
    "    optimizer=mx.optimizer.create('adam', multi_precision=True, learning_rate=args.lr))\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "    loss_function = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    net.cast('float16')\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            # Possibly copy inputs and labels to the GPU\n",
    "            inputs = inputs.astype('float16').as_in_context(ctx)\n",
    "            labels = labels.astype('float16').as_in_context(ctx)\n",
    "    \n",
    "            with autograd.record():\n",
    "                outputs = net(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "    \n",
    "            # Compute gradients by backpropagation and update the evaluation metric\n",
    "            loss.backward()\n",
    "            metric.update(labels, outputs)\n",
    "    \n",
    "            # Update the parameters by stepping the trainer; the batch size\n",
    "            # is required to normalize the gradients by `1 / batch_size`.\n",
    "            trainer.step(batch_size=inputs.shape[0])\n",
    "    \n",
    "        # Print the evaluation metric and reset it for the next epoch\n",
    "        name, acc = metric.get()\n",
    "        print('After epoch {}: {} = {}'.format(epoch + 1, name, acc))\n",
    "        metric.reset()\n",
    "        perf(val_loader, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python VideoClassif.py --backbone resnet18_v2 \\\n",
    "                         --epochs 20 \\\n",
    "                         --ttsplit 0.7 \\\n",
    "                         --frames 15 \\\n",
    "                         --downsample 3 \\\n",
    "                         --batch 16 \\\n",
    "                         --loadworkers 4 \\\n",
    "                         --fc 16 \\\n",
    "                         --lr 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "from sagemaker.mxnet.estimator import MXNet\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "model = MXNet(\n",
    "    entry_point='VideoClassif.py',\n",
    "    py_version='py3',\n",
    "    framework_version='1.3',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    role=get_execution_role(),\n",
    "    metric_definitions=[  # publish algo metrics to Cloudwatch\n",
    "        {'Name': 'train_acc','Regex': \"^.*epoch : accuracy = ([0-9.]+).*$\"},\n",
    "        {'Name': 'test_acc','Regex': \"Test: accuracy: ([0-9.]+).*$\"}],\n",
    "    hyperparameters={\n",
    "        'backbone':'densenet121',\n",
    "        'epochs':'20',\n",
    "        'ttsplit':'0',\n",
    "        'frames':'15',\n",
    "        'downsample':'3',\n",
    "        'batch':'8',\n",
    "        'loadworkers':'4',\n",
    "        'fc':'16',\n",
    "        'lr':'0.001'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune\n",
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "\n",
    "# Define exploration boundaries\n",
    "hyperparameter_ranges = {\n",
    "    'backbone': CategoricalParameter(['resnet18_v2', 'resnet152_v2', 'mobilenetv2_1.0'])\n",
    "    'lr': ContinuousParameter(0.0005, 0.01),\n",
    "    'dropout': ContinuousParameter(0.2, 0.8),\n",
    "    'fc': IntegerParameter(8, 64),\n",
    "    'frames': IntegerParameter(5, 20),\n",
    "    'downsample': IntegerParameter(1, 4),\n",
    "    'batch': IntegerParameter(6, 32)}\n",
    "\n",
    "# create Optimizer\n",
    "Optimizer = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=model,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    base_tuning_job_name='HPO-vid-classifier',\n",
    "    objective_type='Maximize',\n",
    "    objective_metric_name='test_acc',\n",
    "    metric_definitions=[{'Name': 'test_acc','Regex': \"Test: accuracy: ([0-9.]+).*$\"}],    \n",
    "    max_jobs=300,\n",
    "    max_parallel_jobs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
